{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010520d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45ce50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.26708237131886337\n",
      "Epoch 100 Loss: 0.25394366009294034\n",
      "Epoch 200 Loss: 0.2523340533023076\n",
      "Epoch 300 Loss: 0.2512426005478286\n",
      "Epoch 400 Loss: 0.25036728765016053\n",
      "Epoch 500 Loss: 0.24957756378902157\n",
      "Epoch 600 Loss: 0.24877560512604918\n",
      "Epoch 700 Loss: 0.2478726355905278\n",
      "Epoch 800 Loss: 0.24677342908472483\n",
      "Epoch 900 Loss: 0.24536668681393556\n",
      "Epoch 1000 Loss: 0.2435240767215585\n",
      "Epoch 1100 Loss: 0.2411131095605032\n",
      "Epoch 1200 Loss: 0.23802495673776253\n",
      "Epoch 1300 Loss: 0.23420634486374708\n",
      "Epoch 1400 Loss: 0.22967641690376156\n",
      "Epoch 1500 Loss: 0.22451924760832104\n",
      "Epoch 1600 Loss: 0.21886128460284265\n",
      "Epoch 1700 Loss: 0.2128484134125685\n",
      "Epoch 1800 Loss: 0.20662898183189776\n",
      "Epoch 1900 Loss: 0.20034161628783412\n",
      "Epoch 2000 Loss: 0.19410585422979287\n",
      "Epoch 2100 Loss: 0.18801556958620175\n",
      "Epoch 2200 Loss: 0.18213599530990168\n",
      "Epoch 2300 Loss: 0.17650441216673116\n",
      "Epoch 2400 Loss: 0.1711334992920971\n",
      "Epoch 2500 Loss: 0.1660158283923258\n",
      "Epoch 2600 Loss: 0.16112810875001063\n",
      "Epoch 2700 Loss: 0.15643422405020652\n",
      "Epoch 2800 Loss: 0.151886548223387\n",
      "Epoch 2900 Loss: 0.14742538332232238\n",
      "Epoch 3000 Loss: 0.1429766910162929\n",
      "Epoch 3100 Loss: 0.138448756881829\n",
      "Epoch 3200 Loss: 0.13372925721287626\n",
      "Epoch 3300 Loss: 0.12868558249083573\n",
      "Epoch 3400 Loss: 0.12317306167342906\n",
      "Epoch 3500 Loss: 0.11705674064401819\n",
      "Epoch 3600 Loss: 0.11024964844033\n",
      "Epoch 3700 Loss: 0.10276056764553022\n",
      "Epoch 3800 Loss: 0.09472917335997838\n",
      "Epoch 3900 Loss: 0.08641992732054984\n",
      "Epoch 4000 Loss: 0.0781647184812956\n",
      "Epoch 4100 Loss: 0.07027963930431504\n",
      "Epoch 4200 Loss: 0.06299871454184819\n",
      "Epoch 4300 Loss: 0.05644959778974014\n",
      "Epoch 4400 Loss: 0.050666046990737484\n",
      "Epoch 4500 Loss: 0.04561715553969189\n",
      "Epoch 4600 Loss: 0.041236539490755925\n",
      "Epoch 4700 Loss: 0.03744394014331219\n",
      "Epoch 4800 Loss: 0.034158449344559576\n",
      "Epoch 4900 Loss: 0.03130533942008804\n",
      "Epoch 5000 Loss: 0.028818831995800438\n",
      "Epoch 5100 Loss: 0.026642602163146744\n",
      "Epoch 5200 Loss: 0.024729171704873307\n",
      "Epoch 5300 Loss: 0.023038854292648382\n",
      "Epoch 5400 Loss: 0.021538600615874703\n",
      "Epoch 5500 Loss: 0.020200908281949953\n",
      "Epoch 5600 Loss: 0.019002862193770127\n",
      "Epoch 5700 Loss: 0.017925320963014347\n",
      "Epoch 5800 Loss: 0.016952241700163833\n",
      "Epoch 5900 Loss: 0.01607012638498005\n",
      "Epoch 6000 Loss: 0.015267570847313993\n",
      "Epoch 6100 Loss: 0.014534898387550111\n",
      "Epoch 6200 Loss: 0.013863862297267628\n",
      "Epoch 6300 Loss: 0.01324740405738781\n",
      "Epoch 6400 Loss: 0.012679456367031241\n",
      "Epoch 6500 Loss: 0.012154782228628636\n",
      "Epoch 6600 Loss: 0.011668843047692045\n",
      "Epoch 6700 Loss: 0.011217690119861004\n",
      "Epoch 6800 Loss: 0.010797875015468071\n",
      "Epoch 6900 Loss: 0.010406375279308811\n",
      "Epoch 7000 Loss: 0.010040532583789948\n",
      "Epoch 7100 Loss: 0.009698001044491521\n",
      "Epoch 7200 Loss: 0.009376703859331087\n",
      "Epoch 7300 Loss: 0.009074796790958147\n",
      "Epoch 7400 Loss: 0.0087906372966471\n",
      "Epoch 7500 Loss: 0.008522758336516254\n",
      "Epoch 7600 Loss: 0.008269846071727583\n",
      "Epoch 7700 Loss: 0.008030720809088282\n",
      "Epoch 7800 Loss: 0.007804320664744879\n",
      "Epoch 7900 Loss: 0.00758968751336268\n",
      "Epoch 8000 Loss: 0.00738595486495479\n",
      "Epoch 8100 Loss: 0.007192337373011878\n",
      "Epoch 8200 Loss: 0.007008121727654616\n",
      "Epoch 8300 Loss: 0.006832658728442708\n",
      "Epoch 8400 Loss: 0.006665356365023309\n",
      "Epoch 8500 Loss: 0.006505673761401176\n",
      "Epoch 8600 Loss: 0.006353115862396003\n",
      "Epoch 8700 Loss: 0.006207228759721892\n",
      "Epoch 8800 Loss: 0.006067595570800367\n",
      "Epoch 8900 Loss: 0.005933832796483502\n",
      "Epoch 9000 Loss: 0.0058055870947884974\n",
      "Epoch 9100 Loss: 0.005682532416901271\n",
      "Epoch 9200 Loss: 0.005564367459411175\n",
      "Epoch 9300 Loss: 0.0054508133932327715\n",
      "Epoch 9400 Loss: 0.005341611835164519\n",
      "Epoch 9500 Loss: 0.005236523032691304\n",
      "Epoch 9600 Loss: 0.005135324236598678\n",
      "Epoch 9700 Loss: 0.005037808239340985\n",
      "Epoch 9800 Loss: 0.004943782059990663\n",
      "Epoch 9900 Loss: 0.00485306575906582\n",
      "\n",
      "Predictions after training:\n",
      "[[0.03922618]\n",
      " [0.9275694 ]\n",
      " [0.92757951]\n",
      " [0.08385913]]\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Mean Squared Error Loss\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Training the Neural Network\n",
    "def train(X, y, hidden_layer_neurons, epochs, learning_rate):\n",
    "    # Initialize weights randomly with mean 0\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    \n",
    "    # Weights for input to hidden layer\n",
    "    W1 = np.random.randn(input_size, hidden_layer_neurons)\n",
    "    # Weights for hidden layer to output\n",
    "    W2 = np.random.randn(hidden_layer_neurons, output_size)\n",
    "\n",
    "    # Training the network\n",
    "    for epoch in range(epochs):\n",
    "        # Feedforward\n",
    "        hidden_layer_input = np.dot(X, W1)\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        \n",
    "        output_layer_input = np.dot(hidden_layer_output, W2)\n",
    "        output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "        # Calculate the error (loss)\n",
    "        loss = mse_loss(y, output_layer_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Calculate error in output\n",
    "        output_error = y - output_layer_output\n",
    "        output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
    "        \n",
    "        # Calculate error in hidden layer\n",
    "        hidden_error = output_delta.dot(W2.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "        # Update the weights\n",
    "        W2 += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "        W1 += X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch} Loss: {loss}')\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# Predict function\n",
    "def predict(X, W1, W2):\n",
    "    hidden_layer_input = np.dot(X, W1)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    output_layer_input = np.dot(hidden_layer_output, W2)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    return output_layer_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input data (4 samples, 2 features each)\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "    # Output labels (XOR problem)\n",
    "    y = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "\n",
    "    # Train the neural network\n",
    "    hidden_neurons = 4\n",
    "    epochs = 10000\n",
    "    learning_rate = 0.1\n",
    "    W1, W2 = train(X, y, hidden_neurons, epochs, learning_rate)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict(X, W1, W2)\n",
    "    print(\"\\nPredictions after training:\")\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f74bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
